{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLD06oBNVJS1KiQa2w02AT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Savvythelegend/MLE/blob/main/projects/Time-Series%20Data%20Engineering/hf_data_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timeseries Data Handling\n",
        "\n",
        "*   **Time-Series Data:** now i can understand the unique nature of time-series data, which is data collected at regular intervals, often at high frequencies (like every 5ms).\n",
        "*   **Data Transformation:** mastered the critical process of transforming data from a wide format (one row, many columns) to a long format (many rows, one column for each type of value). This is often called \"melting\" or \"unpivoting\" in Pandas and is essential for working with sensor data.\n",
        "*   **Aggregation:** learned how to use aggregation functions, eg. MAX, to reduce huge volumes of data into a more manageable and meaningful summary. This is a common practice in Industrial IoT (IIoT) to prevent data overload.\n",
        "*   **Data Pipelining:** By combining these stepsâ€”reading the data, melting, mapping metadata, aggregating, and formatting the output built a small but complete data pipeline."
      ],
      "metadata": {
        "id": "QDGMmWr7u1xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_iiot_data(file_path: str, output_path: str = \"Sample_Output.csv\"):\n",
        "    \"\"\"\n",
        "    Transforms IIoT sensor data from a wide, high-frequency format to a long,\n",
        "    aggregated time-series.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the input CSV file containing metadata and\n",
        "                         high-frequency sensor readings.\n",
        "        output_path (str): The file path to save the final aggregated CSV.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"Error: The file '{file_path}' was not found.\")\n",
        "        return\n",
        "\n",
        "    ## 1. READ METADATA & DATA\n",
        "    # Dynamically detect the header line to separate metadata from readings\n",
        "    with open(file_path, 'r') as f:\n",
        "        header_line = None\n",
        "        for i, line in enumerate(f):\n",
        "            if 'Date' in line and 'Time' in line:\n",
        "                header_line = i\n",
        "                break\n",
        "\n",
        "    # Read metadata from the top of the file down to the header line\n",
        "    meta = pd.read_csv(file_path, nrows=header_line)\n",
        "    meta = meta.loc[:, ~meta.columns.str.contains('^Unnamed')]\n",
        "\n",
        "    # Read high-frequency data from the header line onwards\n",
        "    hf_data = pd.read_csv(file_path, skiprows=header_line)\n",
        "\n",
        "    ## 2. CREATE MAPPING\n",
        "    # Use the metadata to create lookup dictionaries for each sensor\n",
        "    itemid_map = dict(zip(meta[\"ItemName\"], meta[\"ItemId\"]))\n",
        "    desc_map   = dict(zip(meta[\"ItemName\"], meta[\"Comment\"]))\n",
        "    unit_map   = dict(zip(meta[\"ItemName\"], meta[\"Unit\"]))\n",
        "\n",
        "    ## 3. CONVERT TO LONG FORMAT (MELT)\n",
        "    # Combine Date, Time, Milli Sec into a single timestamp\n",
        "    hf_data['timestamp'] = pd.to_datetime(hf_data['Date'] + ' ' + hf_data['Time'], dayfirst=True) + pd.to_timedelta(hf_data['Milli Sec'], unit='ms')\n",
        "\n",
        "    # Get a list of only the sensor columns\n",
        "    sensor_cols = [col for col in hf_data.columns if col not in ['Date', 'Time', 'Milli Sec', 'timestamp']]\n",
        "\n",
        "    # Melt the DataFrame from a wide format to a long format\n",
        "    hf_long = hf_data.melt(\n",
        "        id_vars=['timestamp'],\n",
        "        value_vars=sensor_cols,\n",
        "        var_name='tag_name',\n",
        "        value_name='tag_value'\n",
        "    )\n",
        "\n",
        "    ## 4. MAP METADATA TO THE LONG DATAFRAME\n",
        "    # Attach the metadata to each sensor reading using the mapping dictionaries\n",
        "    hf_long['tag__id'] = hf_long['tag_name'].map(itemid_map)\n",
        "    hf_long['tag__desc'] = hf_long['tag_name'].map(desc_map)\n",
        "    hf_long['tag__unit'] = 'mps'\n",
        "\n",
        "    ## 5.Create a 10-second time bucket for grouping\n",
        "    hf_long['time_bucket'] = hf_long['timestamp'].dt.floor('10s')\n",
        "\n",
        "    # Group by the time bucket and sensor metadata, then aggregate using MAX\n",
        "    agg = (\n",
        "        hf_long\n",
        "        .groupby(['time_bucket', 'tag__id', 'tag_name', 'tag__desc', 'tag__unit'], as_index=False)\n",
        "        .agg(tag__value=('tag_value', 'max'))\n",
        "    )\n",
        "\n",
        "    ## 6.Format the time_bucket to match the required output timestamp format\n",
        "    agg['event_timestamp'] = agg['time_bucket'].dt.strftime('%d/%m/%Y %H:%M:%S')\n",
        "\n",
        "    out = agg[['event_timestamp', 'tag__id', 'tag_name', 'tag__desc', 'tag__value', 'tag__unit']]\n",
        "\n",
        "    # Export the final DataFrame\n",
        "    out.to_csv(output_path, index=False)\n",
        "    print(\"Transformation complete. Output saved as {}\".format(output_path))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = \"/content/Sample Input - Sample.csv\"\n",
        "\n",
        "    # Run the data processing pipeline\n",
        "    process_iiot_data(input_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0XE5TPZX7u2",
        "outputId": "e0a9593f-8151-496a-e685-f1eddf7cd129"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation complete. Output saved as Sample_Output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjurqzQFJq3c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}